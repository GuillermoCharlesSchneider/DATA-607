---
title: "sentiment_analysis"
author: "GuillermoSchneider"
date: "2024-03-08"
output: html_document
---

```{r}
#install.packages("tidytext")
#install.packages("textdata")
#install.packages("tidyverse")
#install.packages("wordcloud")
library(tidytext)
library(textdata)
library(tidyverse)
library(wordcloud)

```

##https://www.tidytextmining.com/sentiment


**AFINN from Finn Ã…rup Nielsen**
```{r}
get_sentiments("afinn")
```

**bing from Bing Liu and collaborators**
```{r}
get_sentiments("bing")
```

**nrc from Saif Mohammad and Peter Turney**
```{r}
get_sentiments("nrc")
```


**sample of 8000 posts from lucas**
```{r}
test_scrape <- read.csv("https://raw.githubusercontent.com/GuillermoCharlesSchneider/DATA-607/main/Project-3/test_scrape.csv")
```

**separate 8000 comments into single words**
```{r}
tidy_test_scrape <- test_scrape %>%
  unnest_tokens(word, comment)
```

**full 125k posts from lucas**
```{r}
full_scrape <- read.csv("https://raw.githubusercontent.com/riverar9/cuny-msds-project-3/main/part_2/reddit%20scrape.csv")
```

**separate 125k comments into single words**
```{r}
tidy_full_scrape <- full_scrape %>%
  unnest_tokens(word, comment)
```

**Skills provided by jon:**
```{r}
Skills <- read.csv("https://raw.githubusercontent.com/GuillermoCharlesSchneider/DATA-607/main/Project-3/skills.csv")
```

**separate skills into single words, while still keeping the general skill bucket**
```{r}
Skills$word <- tolower(Skills$Skills)

Skills_single_word <- Skills %>%
  unnest_tokens(word, word)

Skills_single_word <- Skills_single_word[-c(4,9,10,11,20,21,22,23,25,28,31,40,41,42,43,46,48,50,55,57,62,65,67,71,72,75,80,83,84,92,99,136,140,141,146,147,148,150,151,152,153,154), ]
row.names(Skills_single_word) <- 1:nrow(Skills_single_word)

```

**TWO WORD SKILLS MERGED INTO ONE (ex:data science -> datascience), multiple words per bucket. this is a stupid method, but i was too tired to figure out a new function, and i needed to look through them all anyway to see which needed to be combined and which didnt:**
```{r}
for(x in c(1,5,10,13,17,21,24,23,26,29,35,37,39,42,46,48,50,56,60,59,64,63,66,73,72,75,77,80,79,82,84,86,89,91,93,98,97,102,101,100,106,108,110)){
  Skills_single_word$word[x] <- paste(Skills_single_word$word[x],Skills_single_word$word[x+1],sep = "")
}

Skills_single_word <- Skills_single_word[-c(2,6,11,14,18,22,25,24,27,30,36,38,40,43,47,49,51,57,61,60,65,64,67,74,73,76,78,81,80,83,85,87,90,92,94,99,98,103,102,101,107,109,111), ]

```

**count of single word Skills words in the tidied scape:**
```{r}
#8000 comments
tidy_test_scrape %>%
  inner_join(Skills_single_word) %>%
  count(word, sort = TRUE)

#125k comments
tidy_full_scrape %>%
  inner_join(Skills_single_word) %>%
  count(word, sort = TRUE)
```


**joyful words from NRC:**
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")
```

**count of joyful words in the tidied single word scrape, matching 1 to 1 words w inner join on "word" column:**
```{r}
tidy_test_scrape %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```



**count of all single words**
```{r}
t <- tidy_scrape %>%
  count(word, sort = TRUE)
```


search through reddit comments, group the skills thatre are 2 words

skills one word
Search for comments that include those words
see what positive or negative words are around in those comments
score?
see which topics have more negative postive score?
weight it by the relevancy?



nrc from Saif Mohammad and Peter Turney -> word cloud


sentiment analysis over time, we have the reddit dates, positive minus negative

list the courses that we scraped

**????? idk i cant this stupid wordcloud to work atm, ill fix it at some pt**
```{r}
library(wordcloud)
w <- sort(rowSums(t), decreasing = TRUE)
set.seed(222)
wordcloud(word = word(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)
```

